{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlw3OJF752YpRcyhtvIZ8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbedle/DISC_NLPwrkshp/blob/main/DISC_workshop_Feb2024_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background information"
      ],
      "metadata": {
        "id": "-opQHDH_MQ_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For today's workshop, we will being doing some basic visualizations and analysis of text using Natural Language Processing techniques.\n",
        "\n",
        "One thing to always keep in mind is that machine learning techniques augment our analyses - and that many techniques exist, and depending on the task you are trying to accomplish  your task will require different methods.   I highly recommend the text '*Text as Data*' by Grimmer et al. if you really want to dig into NLP method more.\n",
        "\n",
        "With any data analysis, it is important to consider the selection of your data (text) and any biases it may have when you are building your text documents (called corpus) to analyze.\n",
        "\n",
        "Today, we are going to start with a techinque called **'Bag of Words'** because we are treating our corpus as a string of individual words.\n",
        "\n",
        "**So the basic steps to get started are:**\n",
        "\n",
        "1.   **Choose your unit of analysis**... is it news headlines, paragraphs in political speeches, tweets?\n",
        "\n",
        "2.   **Tokenize**- this is how you will break down your document.  Typically we are looking at individual words [unigram], but you could have phrases that are important like \"white house\" and want to include bigrams [two word phrases].\n",
        "\n",
        "3. **Reduce the complexity** of your text/words:\n",
        "  - Make all text lowercase\n",
        "  - Remove punctuation\n",
        "  - Remove stop words [common words]\n",
        "  - Create equivalent word classes\n",
        "    - lemmentization\n",
        "    - stemming\n",
        "  - Filter words by frequency\n",
        "\n",
        "4. Can then look at **Representations from language sequencing**, this can be things such as:\n",
        "  - Parts of Speech tagging [POS]\n",
        "    - are you interested in nouns? verbs? adjectives before a noun?\n",
        "  - Named entity recognition [NER]\n",
        "\n",
        "5. Looking at the words in multidimensional space in **Distributed Representation of Words** with **word embeddings**, **vector space modeling**\n",
        "\n",
        "6. **Clustering** methods - these take in the text of each document, and then output each document into 'n' categories.  Common starting methods include k-means.\n",
        "\n",
        "7. **Topic Modeling** - similar to clustering, but allows multiple topics per document.   \n",
        "  - Latent Dirichiet Allocation [LDA] is one of the most popular methods for topic modeling."
      ],
      "metadata": {
        "id": "XQzCbq7izRl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODAY** - Using data from OU's CRCM National 2018 survey: http://crcm.ou.edu/epscordata/\n",
        "\n",
        "**Codebook**:\n",
        "\n",
        "  **Gender** 0 =female, 1 = male\n",
        "  \n",
        "  **glbcc** : In your view, are greenhouse gases, such as those resulting from the combustion of coal, oil, natural gas, and other materials, causing average global temperatures to rise?\n",
        "        0 = No, = 1  Yes\n",
        "\n",
        "  **glbcc_change**: In the last 5 years have you changed your beliefs about whether humans are/are not causing global climate change?\n",
        "        0 = No, 1=Yes, 2=Don't know\n",
        "\n",
        "  **party_w_lean** = 1 = democratic, 2=Republican  NA= other\n",
        "\n",
        "  **glbcc_change_led1**:  - [only IF glbcc_change = 0 or 2] What led you to be more or less condifent in your climate change views than 5 years ago?\n",
        "\n",
        "  **glbcc_change_led2**:  - [only IF glbcc_change = 1]  What led you to changeyour views about whether humans are causing global climate change?"
      ],
      "metadata": {
        "id": "6vw_NhvRL2QI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A couple of Google Colab shortcuts**\n",
        "\n",
        "to **comment/ uncomment** out a section of code - highlight and press Ctrl + /\n",
        "other shortcuts can be found in colab menu under 'Tools -> Keyboard Shortcuts'"
      ],
      "metadata": {
        "id": "KEJa4qcdbuSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all needed python packages and the data"
      ],
      "metadata": {
        "id": "hVXLOaUeMMyF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PJw_73ULlo7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "from collections import Counter\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import spacy\n",
        "from gensim.models import CoherenceModel\n",
        "#from gensim.models.ldamodel import LdaModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK (Natural Language toolikt) resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "s1l9ECIeLrU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "zXVImgyZNS9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################  YOU WILL NEED TO CHANGE THE FILE PATH TO POINT TO YOUR EXCEL #####################################\n",
        "# Read the Excel sheet containing text data\n",
        "file_path =  '/content/gdrive/MyDrive/data_examples/CRCM_glbcc_txt.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "40ynGqWbLrh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check out the header and top couple rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Uq7fHYmLOgp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start by visualizing the data in Python"
      ],
      "metadata": {
        "id": "lDySyN1f8ebS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## Pie chart for gender  #########################\n",
        "# Replace numeric values with meaningful labels  & Count the occurrences of each gender\n",
        "gender_counts = df['gender'].replace({0: 'Female', 1: 'Male'}).value_counts()\n",
        "\n",
        "# Define colors for each gender - feel free to pick new colors!\n",
        "gender_colors = {'Female': 'purple', 'Male': 'blue'}\n",
        "\n",
        "#Create a pie chart\n",
        "plt.pie(gender_counts, labels=gender_counts.index, colors=[gender_colors[gender] for gender in gender_counts.index], autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribution of Gender')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qkr0qlHRLrlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head)"
      ],
      "metadata": {
        "id": "s0_mggCsCYkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################### Vertical bar chart for party lean #########################\n",
        "party_counts = df['party_w_lean'].replace({1: 'Democrat', 2: 'Republican', 3: 'Other'}).value_counts()\n",
        "party_colors = {'Other': 'grey', 'Democrat': 'blue', 'Republican': 'red'}\n",
        "\n",
        "# Plot the bar chart with custom colors and counts on top\n",
        "ax = party_counts.plot(kind='bar', color=[party_colors[party] for party in party_counts.index])\n",
        "ax.bar_label(ax.containers[0], fmt='%d', label_type='edge', fontsize=8)  # Add counts on top of each bar\n",
        "plt.title('Party Affiliation Distribution')\n",
        "plt.ylabel('Number of Respondents')\n",
        "plt.xticks(rotation=0)  # Adjust x-axis ticks\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-qygxi_vLrq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################### Horizontal bar chart for beliefs about climate change  #########################\n",
        "beliefs_labels = df['glbcc'].replace({0: 'No', 1: 'Yes'})\n",
        "beliefs_counts = beliefs_labels.value_counts()\n",
        "beliefs_colors = {'No': 'turquoise', 'Yes': 'orange'}\n",
        "\n",
        "# Create a horizontal bar chart\n",
        "fig, ax = plt.subplots()\n",
        "bars = ax.barh(beliefs_counts.index, beliefs_counts, color=[beliefs_colors[belief] for belief in beliefs_counts.index])\n",
        "\n",
        "# Add values at the end of each bar using a for loop\n",
        "for bar in bars:\n",
        "    ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{bar.get_width()}', ha='left', va='center')\n",
        "\n",
        "# Set title and labels\n",
        "plt.title('Beliefs About Climate Change')\n",
        "plt.xlabel('Number of Respondents')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V7QMXNBdPRD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################### NOW LETS COMBINE VARIABLES FOR A FEW MORE VISUALIZATIONS! USING SEABORN  ###########################\n",
        "# Set the style for seaborn\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Map numeric values to meaningful labels - NOTE NOW WE ARE CHANGING THE DATAFRAME FROM NUMBERS TO LABELS!  SO COOL!\n",
        "### Can see this with print(df.head())\n",
        "df['genderFM'] = df['gender'].replace({0: 'Female', 1: 'Male'})\n",
        "df['party_w_leanDRO'] = df['party_w_lean'].replace({1: 'Democrat', 2: 'Republican', 3: 'Other'})\n",
        "df['glbccNY'] = df['glbcc'].replace({0: 'No', 1: 'Yes'})\n",
        "\n",
        "# Create a countplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='party_w_leanDRO', hue='genderFM', data=df, palette={'Female': 'purple', 'Male': 'blue'}, hue_order=['Female', 'Male'])\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Distribution of Gender across Party Affiliation')\n",
        "plt.xlabel('Party Affiliation')\n",
        "plt.ylabel('Number of Respondents')\n",
        "plt.legend(title='Gender')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dmPTVmRePT69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or as an alternative can create a heatmap for the previous information\n",
        "\n",
        "plt.subplots(figsize=(8, 8))\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['party_w_leanDRO'].value_counts()\n",
        "    for x_label, grp in df.groupby('genderFM')\n",
        "})\n",
        "sns.heatmap(df_2dhist, cmap='viridis')\n",
        "plt.title('Distribution of Gender across Party Affiliation')\n",
        "plt.xlabel('genderFM')\n",
        "_ = plt.ylabel('party_w_leanDRO')"
      ],
      "metadata": {
        "id": "ogKW0js-Ro86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## CLIMATE CHANGE BELIEFS BY GENDER ##################\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='glbccNY', hue='genderFM', data=df, palette={'Female': 'purple', 'Male': 'blue'}, hue_order=['Female', 'Male'])\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Beliefs About Climate Change by Gender')\n",
        "plt.xlabel('Beliefs About Climate Change')\n",
        "plt.ylabel('Number of Respondents')\n",
        "plt.legend(title='Gender')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dwQC-RSoPbjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################## HOW MANY HAVE CHANGED THEIR MIND ABOUT CLIMATE CHANGE?  ############################\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='glbcc_change', data=df, palette='viridis')\n",
        "\n",
        "# Customize x-axis labels\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['No', 'Yes', \"Don't know\"])\n",
        "\n",
        "plt.title('Distribution of Changes in Climate Change Beliefs')\n",
        "plt.xlabel('Changes in Beliefs')\n",
        "plt.ylabel('Number of Respondents')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4plDox3DPbb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's look at the textual answers!"
      ],
      "metadata": {
        "id": "PYcNIHDrPflT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the survey answers just for *glbcc_change_led2*"
      ],
      "metadata": {
        "id": "UaOnbxVtPmdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########   FIRST LETS MAKE A WORDCLOUD OF BOTH FREE RESPONSES   ########\n",
        "# If you want to combine text from two variables - do this...\n",
        "# Concatenate the free response columns after converting to strings\n",
        "#free_responses = df['glbcc_change_led1'].astype(str).dropna() + ' ' + df['glbcc_change_led2'].astype(str).dropna()\n",
        "\n",
        "free_responses =  df['glbcc_change_led2'].astype(str).dropna()\n",
        "\n",
        "# Generate Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(free_responses))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Free Responses')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yjXt82VRPbMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's print out the top of free_responses just to see what the data looks like...\n",
        "free_responses[:20]"
      ],
      "metadata": {
        "id": "bR_lSkmI4SUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HMMMMM.... MAYBE WE WANT TO TAKE OUT SOME OF THOSE WORDS.... LETS GO OVER SOME NLP PROCESSING BASICS.....**\n",
        "\n",
        "**Stop words** are common words that are often removed from text data during NLP. These words, such as \"the,\" \"and,\" \"is,\"\n",
        "Removing stop words can help focus on the more meaningful content of the text.\n",
        "\n",
        "**Lemmatization** is the process of reducing words to their base or root form, known as the lemma.\n",
        "It involves removing inflections or variations to standardize words. This helps in grouping together different forms of a word and simplifies an\n",
        "\n",
        "  - *Example: The lemma of the words \"running,\" \"ran,\" and \"runs\" is \"run.\"*\n",
        "\n",
        "**Tokenization** is the process of breaking down a text into individual units, typically words or phrases, known as tokens.\n",
        "These tokens are the building blocks used in NLP tasks. Tokenization is a crucial step in text analysis as it allows computers to understand and process the structure of the text.\n",
        "  - *For example: The sentence \"I love programming in Python\" would be tokenized into individual words: [\"I\", \"love\", \"programming\", \"in\", \"Python\"].*\n",
        "\n",
        "**Stemming** is another text normalization process that involves reducing words to its base or root form, known as the \"stem.\"\n",
        "Unlike lemmatization, stemming may result in a root form that is not a valid word, as it applies a set of rules to chop off prefixes or suffixes from words. The goal of stemming is to group together words with similar meanings and treat them as the same entity, even if the resulting stem is not a complete word.\n",
        "\n",
        "  - *Example-- the stem of \"happiness,\" \"happy,\" and \"happily\" would be \"happi.\"*\n",
        "\n",
        "  - Stemming is often less linguistically precise than lemmatization but can be computationally faster."
      ],
      "metadata": {
        "id": "1RZ-5inJPrFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################  OK NOW LET'S PREPROCESS   ################################################\n",
        "# Copy responses from the 'glbcc_change_led2' column and set as type 'string'\n",
        "df['glbcc_change_led2_NLP'] = df['glbcc_change_led2'].astype(str)\n",
        "\n",
        "# Extract responses from the 'glbcc_change_led2' column and drop blank values\n",
        "responses = df['glbcc_change_led2_NLP'].dropna()\n",
        "\n",
        "# Tokenization and cleaning with lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#this is my custon stop word list - where I can add other words if I want\n",
        "custom_stop_words = set(['na', 'nan'])  # Add your own custom stop words here!\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenization and lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
        "\n",
        "    # Remove custom and NLTK stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words = stop_words.union(custom_stop_words)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply tokenization and cleaning to each survey response\n",
        "responses['Tokens'] = responses.apply(preprocess_text)\n",
        "\n",
        "# put all the tokens in one list\n",
        "all_tokens = [token for sublist in responses['Tokens'].tolist() for token in sublist]"
      ],
      "metadata": {
        "id": "bptHxjV9P_Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(responses['Tokens'].head(20))"
      ],
      "metadata": {
        "id": "RhRLintESrFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what does our tokenized concattenated text look like?\n",
        "all_tokens[:10]"
      ],
      "metadata": {
        "id": "VB3w3fIt4x0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################  Now that we have pre-processed, lets make the word cloud again - notice any differences?   ######\n",
        "# Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_tokens))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('NEW Word Cloud of Responses after NLP preprocessing')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MrXomxqPQDiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################  WHAT ARE SOME OTHER WAYS TO VISUALIZE THE WORDS IN THE FREE RESPONSE?   ##################\n",
        "# Bar chart of most common words  [falls under frequency analysis]\n",
        "fdist = FreqDist(all_tokens)\n",
        "common_words = fdist.most_common(15)\n",
        "common_words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(common_words_df['Word'], common_words_df['Frequency'], color='skyblue')\n",
        "plt.title('Top 15 Most Common Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UO09jheZQDfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### Or you can just print this out to your terminal screen --- the most common words and their frequencies\n",
        "print(common_words_df)"
      ],
      "metadata": {
        "id": "B1Fe75agQDcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens if we look for phrases, not just inidividual words..."
      ],
      "metadata": {
        "id": "npY26Rbivwfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Calculate frequency distribution of all tokens (unigrams)\n",
        "fdist_all_tokens = FreqDist(all_tokens)\n",
        "\n",
        "# Get the top 15 most common unigrams\n",
        "top_15_unigrams = fdist_all_tokens.most_common(15)\n",
        "\n",
        "# Print the top 15 unigrams and their frequencies\n",
        "print(\"Top 15 Unigrams:\")\n",
        "for unigram, frequency in top_15_unigrams:\n",
        "    print(f\"{unigram}: {frequency}\")\n",
        "\n",
        "# Extract all bigrams from the list of tokens\n",
        "all_bigrams = list(ngrams(all_tokens, 2))\n",
        "\n",
        "# Calculate frequency distribution of bigrams\n",
        "fdist_bigrams = FreqDist(all_bigrams)\n",
        "\n",
        "# Get the top 15 most common bigrams\n",
        "top_15_bigrams = fdist_bigrams.most_common(15)\n",
        "\n",
        "# Print the top 15 bigrams and their frequencies\n",
        "print(\"\\nTop 15 Bigrams:\")\n",
        "for bigram, frequency in top_15_bigrams:\n",
        "    print(f\"{' '.join(bigram)}: {frequency}\")\n"
      ],
      "metadata": {
        "id": "UdUFRgeRu_1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of speech (POS)"
      ],
      "metadata": {
        "id": "DwncNgqeIems"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a function to perform POS tagging on a text\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "# Example text\n",
        "text = \"Heather thinks its really fun to sit around and write code.\"\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tagging(text)\n",
        "\n",
        "# Print the POS tags\n",
        "for token, pos_tag in pos_tags:\n",
        "    print(f\"{token}: {pos_tag}\")\n"
      ],
      "metadata": {
        "id": "BNJCrDZDIRXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OK - so now lets do this on our data!\n",
        "#Define a function to extract verbs from text\n",
        "def extract_verbs(text):\n",
        "    doc = nlp(text)\n",
        "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
        "    return verbs\n",
        "\n",
        "# Apply the function to each response in the glbcc_change_led1 column\n",
        "# recall this variable is -- What led you to be more or less condifent in your climate change views than 5 years ago?\n",
        "all_verbs = [verb for response in df['glbcc_change_led1'] for verb in extract_verbs(str(response))]\n",
        "\n",
        "# Count the frequency of each verb\n",
        "verb_counts = Counter(all_verbs)\n",
        "\n",
        "# Get the top 10 most common verbs\n",
        "top_verbs = verb_counts.most_common(10)\n",
        "\n",
        "# Extract verbs and their frequencies\n",
        "verbs, frequencies = zip(*top_verbs)\n",
        "\n",
        "# Plot the top verbs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(verbs, frequencies, color='turquoise')\n",
        "plt.title('Top 10 Verbs in Responses')\n",
        "plt.xlabel('Verbs')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nyOcp48BJwUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can expand this to look at adjective-noun pairs\n",
        "def extract_adj_noun_pairs(text):\n",
        "    doc = nlp(text)\n",
        "    adj_noun_pairs = [(token.text, token.head.text) for token in doc if token.pos_ == 'ADJ' and token.head.pos_ == 'NOUN']\n",
        "    return adj_noun_pairs\n",
        "\n",
        "# Apply the function to each response in the glbcc_change_led2 column\n",
        "all_adj_noun_pairs = [pair for response in df['glbcc_change_led1'] for pair in extract_adj_noun_pairs(str(response))]\n",
        "\n",
        "# Count the frequency of each adjective-noun pair\n",
        "adj_noun_pair_counts = Counter(all_adj_noun_pairs)\n",
        "\n",
        "# Get the top 10 most common adjective-noun pairs\n",
        "top_adj_noun_pairs = adj_noun_pair_counts.most_common(10)\n",
        "\n",
        "# Extract adjective-noun pairs and their frequencies\n",
        "adj_noun_pairs, frequencies = zip(*top_adj_noun_pairs)\n",
        "\n",
        "cmap = plt.cm.inferno\n",
        "\n",
        "# Plot the top adjective-noun pairs\n",
        "plt.figure(figsize=(10, 6))\n",
        "#plt.barh(range(len(adj_noun_pairs)), frequencies, color='coral')  # Use barh for horizontal bars\n",
        "plt.barh(range(len(adj_noun_pairs)), frequencies, color=cmap(np.linspace(0, 1, len(adj_noun_pairs))))\n",
        "plt.yticks(range(len(adj_noun_pairs)), adj_noun_pairs)  # Set y-ticks as adjective-noun pairs\n",
        "plt.title('Top 10 Adjective-Noun Pairs in Responses')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Adjective-Noun Pairs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zH4ni-ZbKCJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "4hqsVRomjwIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######  First, lets only at folks who now believe in climate change  and try to understand why they changed their minds ######\n",
        "\n",
        "# Filter the dataframe based on the condition glbcc_change = 1\n",
        "changed_views_df = df[df['glbcc_change'] == 1]\n",
        "\n",
        "# Extract the glbcc_change_led2 column\n",
        "change_led2_responses = changed_views_df['glbcc_change_led2']\n",
        "\n",
        "# Display the first few responses to understand the data\n",
        "print(change_led2_responses.head())\n",
        "\n",
        "# Filter out NaN values in glbcc_change_led2\n",
        "valid_responses = changed_views_df['glbcc_change_led2'].dropna()"
      ],
      "metadata": {
        "id": "VnjB9my_XsNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform NER on each valid response\n",
        "named_entities_list = []\n",
        "for response in valid_responses:\n",
        "    # Process the response with spaCy NLP pipeline\n",
        "    doc = nlp(str(response))\n",
        "\n",
        "    # Extract named entities\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    named_entities_list.extend(named_entities)\n",
        "\n",
        "# Print the named entities\n",
        "print(\"Named Entities:\")\n",
        "for entity, label in named_entities_list:\n",
        "    print(f\"{entity} - {label}\")\n",
        "\n",
        "# Create a DataFrame from the named_entities_list\n",
        "df_named_entities = pd.DataFrame(named_entities_list, columns=['Entity', 'Type'])\n",
        "\n",
        "## Note in the print out below that it is far from perfect - ie \"Milder Winters\" is a person?"
      ],
      "metadata": {
        "id": "ltFbweA3j08y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each entity type\n",
        "entity_type_counts = Counter(df_named_entities['Type'])\n",
        "\n",
        "# Plot a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(entity_type_counts.keys(), entity_type_counts.values())\n",
        "plt.title('Named Entity Types')\n",
        "plt.xlabel('Entity Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VzV35KjAYKQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the named entities in the geo-political entity category\n",
        "print(\"Named Entities in GPE category:\")\n",
        "for entity, label in named_entities_list:\n",
        "    if label == 'GPE':\n",
        "        print(f\"{entity} - {label}\")"
      ],
      "metadata": {
        "id": "PYyyVSkQY0Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means clustering"
      ],
      "metadata": {
        "id": "1WWvGyyOZLOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-means clustering** is a popular unsupervised machine learning algorithm used for clustering data points into groups or clusters based on their similarities.\n",
        "\n",
        "The goal of k-means clustering is to partition the data into a predefined number of clusters, with each cluster represented by its centroid (the mean of all data points in the cluster). The algorithm works iteratively by first randomly initializing cluster centroids and then assigning each data point to the nearest centroid. After the initial assignment, the centroids are updated based on the mean of the data points assigned to each cluster. This process repeats until the centroids no longer change significantly, indicating convergence. K-means clustering is widely used in various applications such as customer segmentation, image compression, and anomaly detection."
      ],
      "metadata": {
        "id": "ncDJvdnPZKVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First lets look at the data we want to understand\n",
        "print(valid_responses.head())\n",
        "print('  ')\n",
        "print('The number of valid responses is:')\n",
        "print(valid_responses.size)"
      ],
      "metadata": {
        "id": "QGk5SiataGYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Convert all responses to string type\n",
        "valid_responses = valid_responses.astype(str)\n",
        "\n",
        "# Define the number of clusters\n",
        "num_clusters = 5\n",
        "\n",
        "# Define the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
        "\n",
        "# Define the KMeans clustering model with n_init set explicitly\n",
        "kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
        "\n",
        "# Create a pipeline with TF-IDF vectorizer, TruncatedSVD (for dimensionality reduction), and KMeans\n",
        "pipeline = make_pipeline(tfidf_vectorizer, TruncatedSVD(n_components=50), kmeans)\n",
        "\n",
        "# Fit the pipeline to the valid responses\n",
        "pipeline.fit(valid_responses)\n",
        "\n",
        "# Get the cluster labels\n",
        "cluster_labels = pipeline.predict(valid_responses)\n",
        "\n",
        "# Print the cluster labels\n",
        "print(\"Cluster labels:\")\n",
        "print(cluster_labels)\n"
      ],
      "metadata": {
        "id": "MiT40yYEZaGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or can print our cluster label next to responses:\n",
        "\n",
        "# Print cluster labels along with corresponding responses for top 20\n",
        "for index, label in enumerate(cluster_labels[:20]):\n",
        "    print(f\"Cluster {label}: {valid_responses.iloc[index]}\")\n",
        "\n",
        "#But if in your own dataset you want to print out all of them - uncomment and use code below:\n",
        "# for i, response in enumerate(valid_responses):\n",
        "#     print(f\"Cluster {cluster_labels[i]}: {response}\")"
      ],
      "metadata": {
        "id": "zdDW6EB4bK5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out top 20 comments in Cluster 4\n",
        "cluster_4_responses = valid_responses[cluster_labels == 4]\n",
        "\n",
        "for i, response in enumerate(cluster_4_responses[:20]):\n",
        "    print(f\"Response {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "Dut_47AscloJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And if you want to save the cluster and response to a spreadsheet\n",
        "# Create a DataFrame with cluster labels and corresponding responses\n",
        "cluster_data = pd.DataFrame({'Cluster Label': cluster_labels, 'Response': valid_responses})\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "cluster_data.to_excel('/content/gdrive/MyDrive/data_examples/cluster_responses_NLPwkshp.xlsx', index=False)"
      ],
      "metadata": {
        "id": "0m0xa-I5iHhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Cluster results in multi-dimensional space"
      ],
      "metadata": {
        "id": "_dnB-8Ujl3Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to look at two different methods:\n",
        "\n",
        "1.   Truncated Singular Value Decomposition (SVD)\n",
        "2.   t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "**So what's the difference?**\n",
        "\n",
        "**SVD** focuses on capturing the overall variance of the data by decomposing it into its principal components. It's suitable for summarizing the **global structure** of the data and reducing its dimensionality while preserving as much information as possible. SVD is often used for tasks like noise reduction, feature extraction, and data compression.\n",
        "\n",
        "**t-SNE** emphasizes the preservation of local structure, aiming to represent nearby points in the high-dimensional space as close together in the low-dimensional space. This makes it effective for visualizing clusters and identifying **local patterns** in the data. It's particularly useful for exploring relationships between nearby data points and uncovering clusters with complex shapes.\n",
        "\n",
        "To apply this to **NLP**:\n",
        "\n",
        "In **natural language processing (NLP) analysis, data often exists in a high-dimensional space where each dimension represents a unique word or feature.** Techniques like t-SNE and SVD help reduce this high-dimensional space to a lower-dimensional space, making it easier to visualize and interpret.\n",
        "\n",
        "When we talk about **global patterns in NLP**, we're referring to the overall structure of the dataset, such as **broad topics or themes that encompass the entire corpus of text**. SVD is particularly useful for capturing these global patterns by decomposing the data into its principal components, which represent the main sources of variation in the text.\n",
        "\n",
        "On the other hand, **local patterns refer to more specific relationships or clusters of words that occur within smaller subsets of the data**. t-SNE excels at preserving these local patterns by focusing on the relationships between nearby data points. In the context of NLP, t-SNE can help reveal clusters of words that are semantically similar or frequently co-occur within documents.\n",
        "\n",
        "So, in **NLP analysis, SVD can provide insights into the overarching structure and main themes of the text, while t-SNE can help uncover more nuanced relationships and clusters of words that exist within smaller sections of the data**."
      ],
      "metadata": {
        "id": "ZJ5S3HOOmKm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clusters are tricky to visualize.\n",
        "\n",
        "First lets try  **Truncated Singular Value Decomposition (SVD)** for dimensionality reduction (because they are clustered in higher dimensional space, and we want to plot it in two dimension)\n",
        "\n",
        "The plot shows each data point as a dot, colored by its assigned cluster, allowing us to observe patterns and structures in the data. Closer dots indicate higher similarity, while separated clusters represent distinct groups within the data."
      ],
      "metadata": {
        "id": "23rv-XzMj4gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Fit TruncatedSVD to the TF-IDF vectors\n",
        "svd = TruncatedSVD(n_components=2)\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(valid_responses)\n",
        "tfidf_vectors_2d = svd.fit_transform(tfidf_vectors)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster_num in range(num_clusters):\n",
        "    plt.scatter(tfidf_vectors_2d[cluster_labels == cluster_num, 0], tfidf_vectors_2d[cluster_labels == cluster_num, 1], label=f'Cluster {cluster_num}', alpha=0.5)\n",
        "plt.title('K-means Clustering Visualization')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sOcAc15XjaxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next - **t-SNE** to project high-dimensional data into a 2D space while preserving the local structure of the data points. Each data point, representing a response, is plotted on a scatter plot, with its position determined by its similarity to other points. Points belonging to different clusters are color-coded, allowing us to observe how well-separated or overlapped the clusters are in the 2D space. This visualization helps us gain insights into the relationships between responses and the effectiveness of the clustering algorithm in distinguishing different groups based on their content."
      ],
      "metadata": {
        "id": "iUpXSH4UlhlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize t-SNE with 2 components (2D space) and random initialization\n",
        "tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "\n",
        "# Fit and transform the TF-IDF vectors to 2D space\n",
        "tfidf_vectors_2d = tsne.fit_transform(tfidf_vectors)\n",
        "\n",
        "# Create a DataFrame with the 2D vectors and cluster labels\n",
        "tsne_df = pd.DataFrame({'X': tfidf_vectors_2d[:, 0], 'Y': tfidf_vectors_2d[:, 1], 'Cluster Label': cluster_labels})\n",
        "\n",
        "# Plot the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=tsne_df, x='X', y='Y', hue='Cluster Label', palette='viridis', alpha=0.7)\n",
        "plt.title('t-SNE Visualization of Cluster Overlap')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qt2Es8hgkp9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster 1 looks very tight - lets print it out to see why:\n",
        "# print out top 20 comments in Cluster 1\n",
        "cluster_1_responses = valid_responses[cluster_labels == 1]\n",
        "\n",
        "for i, response in enumerate(cluster_1_responses[:10]):\n",
        "    print(f\"Response {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "jzm4wAoQoHVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Theme Analysis"
      ],
      "metadata": {
        "id": "ZWejf7y3QONq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that **topic modeling** involves identifying and understanding recurring patterns, topics, or themes within a document.\n",
        "\n",
        "Lots of methods - I'll cover one basic one **LDA**"
      ],
      "metadata": {
        "id": "gprJyp4rQSNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######  First, lets only at folks who now believe in climate change  and try to understand why they changed their minds ######\n",
        "\n",
        "# Filter the dataframe based on the condition glbcc_change = 1\n",
        "changed_views_df = df[df['glbcc_change'] == 1]\n",
        "\n",
        "# Extract the glbcc_change_led2 column\n",
        "change_led2_responses = changed_views_df['glbcc_change_led2']\n",
        "\n",
        "# Display the first few responses to understand the data\n",
        "print(change_led2_responses.head())\n",
        "\n",
        "# Filter out NaN values in glbcc_change_led2\n",
        "valid_responses = changed_views_df['glbcc_change_led2'].dropna()"
      ],
      "metadata": {
        "id": "brph6Ah_QDZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LDA analysis - BASIC TOPIC MODELING**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a probabilistic topic modeling technique designed to reveal hidden themes within a set of documents. Operating as a generative probabilistic model, LDA assumes that documents are composed of a mix of topics, and topics are comprised of words. The algorithm iteratively  assigns topics to words and adjusts probability distributions, ultimately producing the probability distribution of topics for each document and the probability distribution of words for each topic."
      ],
      "metadata": {
        "id": "07-k1NqeQaw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add additional stop words if needed\n",
        "custom_stop_words = set(stopwords.words('english'))\n",
        "custom_stop_words.update(['can', 'will'])\n",
        "\n",
        "# Tokenize and preprocess the responses using lemmatization and refined stop word removal\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenized_responses = [\n",
        "    [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(str(response)) if word.isalnum() and word.lower() not in custom_stop_words and len(word) > 2]\n",
        "    for response in valid_responses\n",
        "    if isinstance(response, str)  # Check if the response is a string\n",
        "]\n",
        "\n",
        "# Remove empty sequences\n",
        "tokenized_responses = [tokens for tokens in tokenized_responses if len(tokens) > 0]\n",
        "\n",
        "# Check if there are any valid tokenized responses\n",
        "if not tokenized_responses:\n",
        "    print(\"No valid tokenized responses to analyze.\")\n",
        "else:\n",
        "    # Create a dictionary representation of the documents\n",
        "    dictionary = corpora.Dictionary(tokenized_responses)\n",
        "\n",
        "    # Create a bag-of-words representation of the documents\n",
        "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_responses]\n",
        "\n",
        "    # Train the LDA model\n",
        "    lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "    # Print the topics and their keywords  note usually start with 3 to 5 keywords\n",
        "    topics = lda_model.show_topics(num_topics=5, num_words=4, formatted=False)\n",
        "    for topic_num, keywords in topics:\n",
        "        print(f\"Topic {topic_num + 1}: {', '.join([word[0] for word in keywords])}\")\n",
        "\n",
        "    # Compute coherence score using C_v coherence which is a metric used to evaluate topic coherence by measuring the semantic\n",
        "    # similarity between the top words within each topic. Higher C_v coherence scores indicate more coherent and interpretable\n",
        "    # topics in topic modeling results.\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_responses, corpus=corpus, coherence='c_v')\n",
        "    cv_coherence = coherence_model.get_coherence()\n",
        "\n",
        "    print(f\"C_v Coherence Score: {cv_coherence}\")"
      ],
      "metadata": {
        "id": "f_daZuVp6wy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scientific evidence seems to be a theme - lets look at those folks."
      ],
      "metadata": {
        "id": "AwVEWc27CAkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side note --- I've chatted with some NLP folks on campus and they often use LLMs (like ChatGPT) to provide themes.  Of course... this depends on the privacy of your data.\n",
        "\n",
        "**OK but let's work with just these words...**"
      ],
      "metadata": {
        "id": "tBb36wuIBaCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, lets do this with science as a key word\n",
        "\n",
        "# Filter the DataFrame to include only responses mentioning 'science' in glbcc_change_led2\n",
        "science_responses_df = df[df['glbcc_change_led2'].str.contains('science', case=False, na=False)].copy()\n",
        "\n",
        "# Replace numeric values with meaningful labels for gender\n",
        "science_responses_df.loc[:, 'gender'] = science_responses_df['gender'].replace({0: 'Female', 1: 'Male'})\n",
        "\n",
        "# Replace numeric values with meaningful labels for party affiliation\n",
        "science_responses_df.loc[:, 'party_w_lean'] = science_responses_df['party_w_lean'].replace({1: 'Democrat', 2: 'Republican', 3: 'Other'})\n",
        "\n",
        "# Create a new column to represent combinations of gender and party affiliation\n",
        "science_responses_df.loc[:, 'gender_party'] = science_responses_df['gender'] + ' ' + science_responses_df['party_w_lean']\n",
        "\n",
        "# Count the occurrences of each combination of gender and party affiliation\n",
        "gender_party_counts = science_responses_df['gender_party'].value_counts()\n",
        "\n",
        "# Plot the distribution with specific color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Define colors for each combination of gender and party affiliation\n",
        "colors = {'Male Democrat': 'blue', 'Female Democrat': 'lightblue',\n",
        "          'Male Republican': 'red', 'Female Republican': 'lightcoral',\n",
        "          'Male Other': 'green', 'Female Other': 'lightgreen'}\n",
        "# Update the color dictionary to include all possible combinations\n",
        "colors = {key: colors.get(key, 'gray') for key in gender_party_counts.index}\n",
        "gender_party_counts.plot(kind='bar', color=[colors[x] for x in gender_party_counts.index])\n",
        "plt.title('Distribution of Party Affiliation across Gender for Respondents Mentioning \"Science\"')\n",
        "plt.xlabel('Gender and Party Affiliation')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7hvzO0KcFUFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Obviously we can dice this many ways....  here is looking at it based on the word 'weather'\n",
        "# but as a percentage of folks who changed their climate belifef\n",
        "\n",
        "# Filter the DataFrame to include only respondents who changed their climate beliefs\n",
        "changed_beliefs_df = df[df['glbcc_change'] == 1]\n",
        "\n",
        "# Calculate the total number of respondents who changed their climate beliefs\n",
        "total_changed_beliefs = len(changed_beliefs_df)\n",
        "\n",
        "# Filter the DataFrame to include only responses mentioning 'weather' in glbcc_change_led2\n",
        "weather_responses_df = changed_beliefs_df[changed_beliefs_df['glbcc_change_led2'].str.contains('weather', case=False, na=False)].copy()\n",
        "\n",
        "# Replace numeric values with meaningful labels for gender\n",
        "weather_responses_df.loc[:, 'gender'] = weather_responses_df['gender'].replace({0: 'Female', 1: 'Male'})\n",
        "\n",
        "# Replace numeric values with meaningful labels for party affiliation\n",
        "weather_responses_df.loc[:, 'party_w_lean'] = weather_responses_df['party_w_lean'].replace({1: 'Democrat', 2: 'Republican', 3: 'Other'})\n",
        "\n",
        "# Create a new column to represent combinations of gender and party affiliation\n",
        "weather_responses_df.loc[:, 'gender_party'] = weather_responses_df['gender'] + ' ' + weather_responses_df['party_w_lean']\n",
        "\n",
        "# Count the occurrences of each combination of gender and party affiliation\n",
        "gender_party_counts = weather_responses_df['gender_party'].value_counts()\n",
        "\n",
        "# Calculate the percentages\n",
        "gender_party_percentages = (gender_party_counts / total_changed_beliefs) * 100\n",
        "\n",
        "# Plot the distribution with specific color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Define colors for each combination of gender and party affiliation\n",
        "colors = {'Male Democrat': 'blue', 'Female Democrat': 'lightblue',\n",
        "          'Male Republican': 'red', 'Female Republican': 'lightcoral',\n",
        "          'Male Other': 'green', 'Female Other': 'lightgreen'}\n",
        "# Update the color dictionary to include all possible combinations\n",
        "colors = {key: colors.get(key, 'gray') for key in gender_party_percentages.index}\n",
        "gender_party_percentages.plot(kind='bar', color=[colors[x] for x in gender_party_percentages.index])\n",
        "plt.title('Distribution of Party Affiliation across Gender for Respondents Mentioning \"Weather\" (as a percentage of climate belief changers)')\n",
        "plt.xlabel('Gender and Party Affiliation')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rBig9uPGF7Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ok.. now one last visualization - Co-Occurance Analysis"
      ],
      "metadata": {
        "id": "oygbds4YjkzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **co-occurrence matrix** is a fundamental concept in natural language processing (NLP) that **helps us understand how often words appear together** in a given context.\n",
        "\n",
        "Imagine we have a large collection of text documents, and we want to analyze which words tend to occur together frequently. To create a co-occurrence matrix, we first identify all the unique words in our corpus (vocabulary). Then, for each pair of words, we count how many times they appear together within a specified window of text. The resulting matrix provides a numerical representation of word co-occurrences, where each cell indicates the frequency with which two words appear together."
      ],
      "metadata": {
        "id": "fSM0fNh0pMOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all responses into a single text\n",
        "all_responses = ' '.join(str(response) for response in changed_views_df['glbcc_change_led2'])\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(all_responses)\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "# Calculate word frequencies\n",
        "freq_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Extract the top 20 co-occurring words\n",
        "top_words = [word for word, _ in freq_dist.most_common(20)]\n",
        "\n",
        "# Create a co-occurrence matrix\n",
        "co_occurrence_matrix = pd.DataFrame(index=top_words, columns=top_words, data=0)\n",
        "\n",
        "# Count co-occurrences in the responses\n",
        "for response in changed_views_df['glbcc_change_led2']:\n",
        "    response_tokens = word_tokenize(str(response))\n",
        "    response_tokens = [word.lower() for word in response_tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "    for i, word1 in enumerate(top_words):\n",
        "        if word1 in response_tokens:\n",
        "            for j, word2 in enumerate(top_words):\n",
        "                if j > i and word2 in response_tokens:\n",
        "                    co_occurrence_matrix.at[word1, word2] += 1\n",
        "                    co_occurrence_matrix.at[word2, word1] += 1\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(co_occurrence_matrix, annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "plt.title('Co-occurrence Heatmap of Top 20 Words  WHY PEOPLE CHANGED MIND ABOUT CC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7bLAN8RHjq_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}